{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "NLP_8_Word2Vec_J010.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetbiyani09/NLP/blob/master/NLP_8_Word2Vec_J010.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRiVsp2-8l9e"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DptoMw8l9t"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUuMoXAh8l9u"
      },
      "source": [
        "def tokenize(text):\n",
        "    # obtains tokens with a least 1 alphabet\n",
        "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "    return pattern.findall(text.lower())\n",
        "\n",
        "def mapping(tokens):\n",
        "    word_to_id = dict()\n",
        "    id_to_word = dict()\n",
        "\n",
        "    for i, token in enumerate(set(tokens)):\n",
        "        word_to_id[token] = i\n",
        "        id_to_word[i] = token\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def generate_training_data(tokens, word_to_id, window_size):\n",
        "    N = len(tokens)\n",
        "    X, Y = [], []\n",
        "\n",
        "    for i in range(N):\n",
        "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
        "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
        "        for j in nbr_inds:\n",
        "            X.append(word_to_id[tokens[i]])\n",
        "            Y.append(word_to_id[tokens[j]])\n",
        "            \n",
        "    X = np.array(X)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    Y = np.array(Y)\n",
        "    Y = np.expand_dims(Y, axis=0)\n",
        "            \n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzYWNv3X8l9v"
      },
      "source": [
        "doc = \"After the deduction of the costs of investing, \" \\\n",
        "      \"beating the stock market is a loser's game.\"\n",
        "tokens = tokenize(doc)\n",
        "word_to_id, id_to_word = mapping(tokens)\n",
        "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
        "vocab_size = len(id_to_word)\n",
        "m = Y.shape[1]\n",
        "# turn Y into one hot encoding\n",
        "Y_one_hot = np.zeros((vocab_size, m))\n",
        "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHLA1yg8l9w"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWVjWVVB8l9x"
      },
      "source": [
        "def initialize_wrd_emb(vocab_size, emb_size):\n",
        "    \"\"\"\n",
        "    vocab_size: int. vocabulary size of your corpus or training data\n",
        "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
        "    \"\"\"\n",
        "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
        "    \n",
        "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
        "    return WRD_EMB\n",
        "\n",
        "def initialize_dense(input_size, output_size):\n",
        "    \"\"\"\n",
        "    input_size: int. size of the input to the dense layer\n",
        "    output_szie: int. size of the output out of the dense layer\n",
        "    \"\"\"\n",
        "    W = np.random.randn(output_size, input_size) * 0.01\n",
        "    \n",
        "    assert(W.shape == (output_size, input_size))\n",
        "    return W\n",
        "\n",
        "def initialize_parameters(vocab_size, emb_size):\n",
        "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
        "    W = initialize_dense(emb_size, vocab_size)\n",
        "    \n",
        "    parameters = {}\n",
        "    parameters['WRD_EMB'] = WRD_EMB\n",
        "    parameters['W'] = W\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBfWpyCJ8l9y"
      },
      "source": [
        "## Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-xvMv8_8l9z"
      },
      "source": [
        "def ind_to_word_vecs(inds, parameters):\n",
        "    \"\"\"\n",
        "    inds: numpy array. shape: (1, m)\n",
        "    parameters: dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = inds.shape[1]\n",
        "    WRD_EMB = parameters['WRD_EMB']\n",
        "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
        "    \n",
        "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
        "    \n",
        "    return word_vec\n",
        "\n",
        "def linear_dense(word_vec, parameters):\n",
        "    \"\"\"\n",
        "    word_vec: numpy array. shape: (emb_size, m)\n",
        "    parameters: dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = word_vec.shape[1]\n",
        "    W = parameters['W']\n",
        "    Z = np.dot(W, word_vec)\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], m))\n",
        "    \n",
        "    return W, Z\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
        "    \n",
        "    assert(softmax_out.shape == Z.shape)\n",
        "\n",
        "    return softmax_out\n",
        "\n",
        "def forward_propagation(inds, parameters):\n",
        "    word_vec = ind_to_word_vecs(inds, parameters)\n",
        "    W, Z = linear_dense(word_vec, parameters)\n",
        "    softmax_out = softmax(Z)\n",
        "    \n",
        "    caches = {}\n",
        "    caches['inds'] = inds\n",
        "    caches['word_vec'] = word_vec\n",
        "    caches['W'] = W\n",
        "    caches['Z'] = Z\n",
        "    \n",
        "    return softmax_out, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Ajfq0c8l90"
      },
      "source": [
        "## Cost Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFYfW6Y8l91"
      },
      "source": [
        "def cross_entropy(softmax_out, Y):\n",
        "    \"\"\"\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    m = softmax_out.shape[1]\n",
        "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2sxwdOI8l91"
      },
      "source": [
        "## Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix38Z3m_8l92"
      },
      "source": [
        "def softmax_backward(Y, softmax_out):\n",
        "    \"\"\"\n",
        "    Y: labels of training data. shape: (vocab_size, m)\n",
        "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "    dL_dZ = softmax_out - Y\n",
        "    \n",
        "    assert(dL_dZ.shape == softmax_out.shape)\n",
        "    return dL_dZ\n",
        "\n",
        "def dense_backward(dL_dZ, caches):\n",
        "    \"\"\"\n",
        "    dL_dZ: shape: (vocab_size, m)\n",
        "    caches: dict. results from each steps of forward propagation\n",
        "    \"\"\"\n",
        "    W = caches['W']\n",
        "    word_vec = caches['word_vec']\n",
        "    m = word_vec.shape[1]\n",
        "    \n",
        "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
        "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
        "\n",
        "    assert(W.shape == dL_dW.shape)\n",
        "    assert(word_vec.shape == dL_dword_vec.shape)\n",
        "    \n",
        "    return dL_dW, dL_dword_vec\n",
        "\n",
        "def backward_propagation(Y, softmax_out, caches):\n",
        "    dL_dZ = softmax_backward(Y, softmax_out)\n",
        "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
        "    \n",
        "    gradients = dict()\n",
        "    gradients['dL_dZ'] = dL_dZ\n",
        "    gradients['dL_dW'] = dL_dW\n",
        "    gradients['dL_dword_vec'] = dL_dword_vec\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
        "    inds = caches['inds']\n",
        "    dL_dword_vec = gradients['dL_dword_vec']\n",
        "    m = inds.shape[-1]\n",
        "    \n",
        "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
        "\n",
        "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJbIKlVZ8l92"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    if parameters is None:\n",
        "        parameters = initialize_parameters(vocab_size, emb_size)\n",
        "    \n",
        "    begin_time = datetime.now()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        batch_inds = list(range(0, m, batch_size))\n",
        "        np.random.shuffle(batch_inds)\n",
        "        for i in batch_inds:\n",
        "            X_batch = X[:, i:i+batch_size]\n",
        "            Y_batch = Y[:, i:i+batch_size]\n",
        "\n",
        "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
        "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
        "            update_parameters(parameters, caches, gradients, learning_rate)\n",
        "            cost = cross_entropy(softmax_out, Y_batch)\n",
        "            epoch_cost += np.squeeze(cost)\n",
        "            \n",
        "        costs.append(epoch_cost)\n",
        "        if print_cost and epoch % (epochs // 500) == 0:\n",
        "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
        "        if epoch % (epochs // 100) == 0:\n",
        "            learning_rate *= 0.98\n",
        "    end_time = datetime.now()\n",
        "    print('training time: {}'.format(end_time - begin_time))\n",
        "            \n",
        "    if plot_cost:\n",
        "        plt.plot(np.arange(epochs), costs)\n",
        "        plt.xlabel('# of epochs')\n",
        "        plt.ylabel('cost')\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T4O8Fg4r8l93",
        "outputId": "97112041-b5b6-4789-bfbe-af941f5a081d"
      },
      "source": [
        "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: 2.5521785563304342\n",
            "Cost after epoch 10: 2.5518758907693218\n",
            "Cost after epoch 20: 2.5515632070536856\n",
            "Cost after epoch 30: 2.5512223189598955\n",
            "Cost after epoch 40: 2.5508349427253623\n",
            "Cost after epoch 50: 2.5503815477868343\n",
            "Cost after epoch 60: 2.5498514065602245\n",
            "Cost after epoch 70: 2.5492160018788166\n",
            "Cost after epoch 80: 2.548449273897728\n",
            "Cost after epoch 90: 2.547522766476271\n",
            "Cost after epoch 100: 2.54640399565781\n",
            "Cost after epoch 110: 2.5450822121585612\n",
            "Cost after epoch 120: 2.543502370338508\n",
            "Cost after epoch 130: 2.541616398112852\n",
            "Cost after epoch 140: 2.5393725587527882\n",
            "Cost after epoch 150: 2.536712630219495\n",
            "Cost after epoch 160: 2.5336327856141505\n",
            "Cost after epoch 170: 2.5300297935027456\n",
            "Cost after epoch 180: 2.5258260956069556\n",
            "Cost after epoch 190: 2.5209466433184797\n",
            "Cost after epoch 200: 2.515315896281367\n",
            "Cost after epoch 210: 2.5089854530537194\n",
            "Cost after epoch 220: 2.501817308245169\n",
            "Cost after epoch 230: 2.493756063346432\n",
            "Cost after epoch 240: 2.484782178775638\n",
            "Cost after epoch 250: 2.4749091790157154\n",
            "Cost after epoch 260: 2.464392263643385\n",
            "Cost after epoch 270: 2.4531844224510797\n",
            "Cost after epoch 280: 2.4414118766458075\n",
            "Cost after epoch 290: 2.4292664043595074\n",
            "Cost after epoch 300: 2.4169736623927043\n",
            "Cost after epoch 310: 2.4049915466003484\n",
            "Cost after epoch 320: 2.393340897937183\n",
            "Cost after epoch 330: 2.3821805989208955\n",
            "Cost after epoch 340: 2.3716411473026255\n",
            "Cost after epoch 350: 2.3617893998509305\n",
            "Cost after epoch 360: 2.3527880600191753\n",
            "Cost after epoch 370: 2.344422392878888\n",
            "Cost after epoch 380: 2.336590064282204\n",
            "Cost after epoch 390: 2.3291859172406912\n",
            "Cost after epoch 400: 2.3220951287301728\n",
            "Cost after epoch 410: 2.3153253179071602\n",
            "Cost after epoch 420: 2.308655521798548\n",
            "Cost after epoch 430: 2.3019767759674044\n",
            "Cost after epoch 440: 2.295205290362219\n",
            "Cost after epoch 450: 2.288270449583288\n",
            "Cost after epoch 460: 2.281247421519507\n",
            "Cost after epoch 470: 2.2739899533333903\n",
            "Cost after epoch 480: 2.266461405656522\n",
            "Cost after epoch 490: 2.2586545891910306\n",
            "Cost after epoch 500: 2.250577361142037\n",
            "Cost after epoch 510: 2.2424039390434496\n",
            "Cost after epoch 520: 2.234041679188136\n",
            "Cost after epoch 530: 2.2255179293246217\n",
            "Cost after epoch 540: 2.216885262684524\n",
            "Cost after epoch 550: 2.2082027182733075\n",
            "Cost after epoch 560: 2.199688967861488\n",
            "Cost after epoch 570: 2.191264733893465\n",
            "Cost after epoch 580: 2.1829698563025612\n",
            "Cost after epoch 590: 2.17485655443824\n",
            "Cost after epoch 600: 2.166969874338169\n",
            "Cost after epoch 610: 2.159481517831158\n",
            "Cost after epoch 620: 2.1522874967388654\n",
            "Cost after epoch 630: 2.145390223427674\n",
            "Cost after epoch 640: 2.138800548542338\n",
            "Cost after epoch 650: 2.132523329293177\n",
            "Cost after epoch 660: 2.1266636804932286\n",
            "Cost after epoch 670: 2.121113065190795\n",
            "Cost after epoch 680: 2.1158532680913456\n",
            "Cost after epoch 690: 2.110876473373297\n",
            "Cost after epoch 700: 2.106173475412364\n",
            "Cost after epoch 710: 2.101812057775306\n",
            "Cost after epoch 720: 2.097702625369059\n",
            "Cost after epoch 730: 2.093825261738831\n",
            "Cost after epoch 740: 2.0901689658305274\n",
            "Cost after epoch 750: 2.0867226374578967\n",
            "Cost after epoch 760: 2.083532285556962\n",
            "Cost after epoch 770: 2.080529434944633\n",
            "Cost after epoch 780: 2.0776974137955615\n",
            "Cost after epoch 790: 2.075026642885672\n",
            "Cost after epoch 800: 2.0725079828357957\n",
            "Cost after epoch 810: 2.0701745742743705\n",
            "Cost after epoch 820: 2.0679761882656944\n",
            "Cost after epoch 830: 2.065900741676119\n",
            "Cost after epoch 840: 2.0639415608472005\n",
            "Cost after epoch 850: 2.0620924480781873\n",
            "Cost after epoch 860: 2.060378399711509\n",
            "Cost after epoch 870: 2.0587631371439152\n",
            "Cost after epoch 880: 2.0572383930918745\n",
            "Cost after epoch 890: 2.0557998511966806\n",
            "Cost after epoch 900: 2.054443499603996\n",
            "Cost after epoch 910: 2.05318813499551\n",
            "Cost after epoch 920: 2.0520074839112126\n",
            "Cost after epoch 930: 2.0508958131009623\n",
            "Cost after epoch 940: 2.049850235108629\n",
            "Cost after epoch 950: 2.048868035306612\n",
            "Cost after epoch 960: 2.0479629038632994\n",
            "Cost after epoch 970: 2.047115830260292\n",
            "Cost after epoch 980: 2.04632273346212\n",
            "Cost after epoch 990: 2.0455815668731336\n",
            "Cost after epoch 1000: 2.044890385137722\n",
            "Cost after epoch 1010: 2.0442586675859276\n",
            "Cost after epoch 1020: 2.043672858694551\n",
            "Cost after epoch 1030: 2.0431300026387125\n",
            "Cost after epoch 1040: 2.0426285590077997\n",
            "Cost after epoch 1050: 2.042167042593354\n",
            "Cost after epoch 1060: 2.041751460447739\n",
            "Cost after epoch 1070: 2.0413724300009846\n",
            "Cost after epoch 1080: 2.041027761924524\n",
            "Cost after epoch 1090: 2.0407161968784022\n",
            "Cost after epoch 1100: 2.040436498097027\n",
            "Cost after epoch 1110: 2.040191817593239\n",
            "Cost after epoch 1120: 2.0399759792740073\n",
            "Cost after epoch 1130: 2.0397873069132872\n",
            "Cost after epoch 1140: 2.0396246773189675\n",
            "Cost after epoch 1150: 2.0394869724164137\n",
            "Cost after epoch 1160: 2.0393750662612447\n",
            "Cost after epoch 1170: 2.0392852442337124\n",
            "Cost after epoch 1180: 2.0392161869596124\n",
            "Cost after epoch 1190: 2.0391668443553645\n",
            "Cost after epoch 1200: 2.0391361680388895\n",
            "Cost after epoch 1210: 2.03912333106727\n",
            "Cost after epoch 1220: 2.039126489297399\n",
            "Cost after epoch 1230: 2.0391445993969075\n",
            "Cost after epoch 1240: 2.0391766822413055\n",
            "Cost after epoch 1250: 2.0392217651763294\n",
            "Cost after epoch 1260: 2.0392778803092932\n",
            "Cost after epoch 1270: 2.03934456281563\n",
            "Cost after epoch 1280: 2.039421005535615\n",
            "Cost after epoch 1290: 2.039506325795675\n",
            "Cost after epoch 1300: 2.0395996546582333\n",
            "Cost after epoch 1310: 2.0396983897453485\n",
            "Cost after epoch 1320: 2.039803024331992\n",
            "Cost after epoch 1330: 2.0399129623739154\n",
            "Cost after epoch 1340: 2.0400274469188853\n",
            "Cost after epoch 1350: 2.0401457415915\n",
            "Cost after epoch 1360: 2.040265035187681\n",
            "Cost after epoch 1370: 2.0403864205630273\n",
            "Cost after epoch 1380: 2.040509491544316\n",
            "Cost after epoch 1390: 2.0406336402889997\n",
            "Cost after epoch 1400: 2.040758285089585\n",
            "Cost after epoch 1410: 2.0408807380273153\n",
            "Cost after epoch 1420: 2.04100239895749\n",
            "Cost after epoch 1430: 2.041123031904651\n",
            "Cost after epoch 1440: 2.0412421933181366\n",
            "Cost after epoch 1450: 2.041359469684568\n",
            "Cost after epoch 1460: 2.0414725302933663\n",
            "Cost after epoch 1470: 2.0415828582576045\n",
            "Cost after epoch 1480: 2.041690368514616\n",
            "Cost after epoch 1490: 2.041794786920047\n",
            "Cost after epoch 1500: 2.0418958709071737\n",
            "Cost after epoch 1510: 2.0419917815683974\n",
            "Cost after epoch 1520: 2.0420839282978904\n",
            "Cost after epoch 1530: 2.0421723548981405\n",
            "Cost after epoch 1540: 2.042256948313755\n",
            "Cost after epoch 1550: 2.0423376253582814\n",
            "Cost after epoch 1560: 2.042413077496694\n",
            "Cost after epoch 1570: 2.042484543185673\n",
            "Cost after epoch 1580: 2.0425521664698345\n",
            "Cost after epoch 1590: 2.042615970774009\n",
            "Cost after epoch 1600: 2.042676004012858\n",
            "Cost after epoch 1610: 2.042731442237619\n",
            "Cost after epoch 1620: 2.042783305503603\n",
            "Cost after epoch 1630: 2.042831801423877\n",
            "Cost after epoch 1640: 2.0428770498151207\n",
            "Cost after epoch 1650: 2.0429191865648595\n",
            "Cost after epoch 1660: 2.0429577649103963\n",
            "Cost after epoch 1670: 2.0429935778415413\n",
            "Cost after epoch 1680: 2.0430268548559316\n",
            "Cost after epoch 1690: 2.043057762853107\n",
            "Cost after epoch 1700: 2.0430864751194857\n",
            "Cost after epoch 1710: 2.0431127851658784\n",
            "Cost after epoch 1720: 2.0431372784113733\n",
            "Cost after epoch 1730: 2.043160166789843\n",
            "Cost after epoch 1740: 2.0431816162807763\n",
            "Cost after epoch 1750: 2.0432017906465547\n",
            "Cost after epoch 1760: 2.0432205918186868\n",
            "Cost after epoch 1770: 2.0432384351682122\n",
            "Cost after epoch 1780: 2.043255486241725\n",
            "Cost after epoch 1790: 2.0432718741925755\n",
            "Cost after epoch 1800: 2.043287720363678\n",
            "Cost after epoch 1810: 2.0433029358349586\n",
            "Cost after epoch 1820: 2.04331781152071\n",
            "Cost after epoch 1830: 2.0433324537672757\n",
            "Cost after epoch 1840: 2.043346937530291\n",
            "Cost after epoch 1850: 2.0433613281071823\n",
            "Cost after epoch 1860: 2.0433754922400644\n",
            "Cost after epoch 1870: 2.043389640557183\n",
            "Cost after epoch 1880: 2.0434038233193266\n",
            "Cost after epoch 1890: 2.043418062289491\n",
            "Cost after epoch 1900: 2.0434323709273188\n",
            "Cost after epoch 1910: 2.0434465617115483\n",
            "Cost after epoch 1920: 2.0434608002292287\n",
            "Cost after epoch 1930: 2.0434750949312517\n",
            "Cost after epoch 1940: 2.043489428380282\n",
            "Cost after epoch 1950: 2.0435037781194705\n",
            "Cost after epoch 1960: 2.043517923143917\n",
            "Cost after epoch 1970: 2.0435320065944342\n",
            "Cost after epoch 1980: 2.0435460140553077\n",
            "Cost after epoch 1990: 2.0435599086304497\n",
            "Cost after epoch 2000: 2.043573652273709\n",
            "Cost after epoch 2010: 2.0435870254061115\n",
            "Cost after epoch 2020: 2.043600159218874\n",
            "Cost after epoch 2030: 2.043613034488498\n",
            "Cost after epoch 2040: 2.043625614055222\n",
            "Cost after epoch 2050: 2.0436378630144842\n",
            "Cost after epoch 2060: 2.0436495972556283\n",
            "Cost after epoch 2070: 2.043660937937633\n",
            "Cost after epoch 2080: 2.043671874600678\n",
            "Cost after epoch 2090: 2.0436823841706193\n",
            "Cost after epoch 2100: 2.043692448086174\n",
            "Cost after epoch 2110: 2.043701940155465\n",
            "Cost after epoch 2120: 2.0437109692534765\n",
            "Cost after epoch 2130: 2.0437195409779694\n",
            "Cost after epoch 2140: 2.0437276535591935\n",
            "Cost after epoch 2150: 2.043735310535551\n",
            "Cost after epoch 2160: 2.0437424491058866\n",
            "Cost after epoch 2170: 2.0437491632128584\n",
            "Cost after epoch 2180: 2.0437554753191147\n",
            "Cost after epoch 2190: 2.0437614047134347\n",
            "Cost after epoch 2200: 2.043766975399351\n",
            "Cost after epoch 2210: 2.0437721765872037\n",
            "Cost after epoch 2220: 2.0437770867653686\n",
            "Cost after epoch 2230: 2.0437817408470313\n",
            "Cost after epoch 2240: 2.0437861729773776\n",
            "Cost after epoch 2250: 2.043790420449082\n",
            "Cost after epoch 2260: 2.043794500995409\n",
            "Cost after epoch 2270: 2.0437984804004823\n",
            "Cost after epoch 2280: 2.0438023987227782\n",
            "Cost after epoch 2290: 2.0438062955748886\n",
            "Cost after epoch 2300: 2.0438102117612633\n",
            "Cost after epoch 2310: 2.0438141652278516\n",
            "Cost after epoch 2320: 2.0438182148501602\n",
            "Cost after epoch 2330: 2.043822398194337\n",
            "Cost after epoch 2340: 2.0438267507602266\n",
            "Cost after epoch 2350: 2.0438313074341203\n",
            "Cost after epoch 2360: 2.0438360605568353\n",
            "Cost after epoch 2370: 2.0438410695349565\n",
            "Cost after epoch 2380: 2.043846363405535\n",
            "Cost after epoch 2390: 2.0438519660451218\n",
            "Cost after epoch 2400: 2.0438578994054915\n",
            "Cost after epoch 2410: 2.04386411230492\n",
            "Cost after epoch 2420: 2.043870671688114\n",
            "Cost after epoch 2430: 2.0438775945168093\n",
            "Cost after epoch 2440: 2.0438848886897394\n",
            "Cost after epoch 2450: 2.043892559482704\n",
            "Cost after epoch 2460: 2.04390050488091\n",
            "Cost after epoch 2470: 2.043908803863258\n",
            "Cost after epoch 2480: 2.043917460201139\n",
            "Cost after epoch 2490: 2.043926464639413\n",
            "Cost after epoch 2500: 2.0439358051634007\n",
            "Cost after epoch 2510: 2.043945332435767\n",
            "Cost after epoch 2520: 2.0439551379502463\n",
            "Cost after epoch 2530: 2.0439652129757935\n",
            "Cost after epoch 2540: 2.0439755324647852\n",
            "Cost after epoch 2550: 2.0439860688950917\n",
            "Cost after epoch 2560: 2.0439966383988586\n",
            "Cost after epoch 2570: 2.0440073408096175\n",
            "Cost after epoch 2580: 2.044018156586345\n",
            "Cost after epoch 2590: 2.044029047900894\n",
            "Cost after epoch 2600: 2.0440399749970957\n",
            "Cost after epoch 2610: 2.044050739082623\n",
            "Cost after epoch 2620: 2.0440614397153762\n",
            "Cost after epoch 2630: 2.0440720485418384\n",
            "Cost after epoch 2640: 2.044082518772693\n",
            "Cost after epoch 2650: 2.044092802358275\n",
            "Cost after epoch 2660: 2.0441027092717596\n",
            "Cost after epoch 2670: 2.044112326604227\n",
            "Cost after epoch 2680: 2.04412161900799\n",
            "Cost after epoch 2690: 2.0441305346997303\n",
            "Cost after epoch 2700: 2.0441390213198174\n",
            "Cost after epoch 2710: 2.0441469228224056\n",
            "Cost after epoch 2720: 2.0441542993003576\n",
            "Cost after epoch 2730: 2.0441611097895187\n",
            "Cost after epoch 2740: 2.0441673011822563\n",
            "Cost after epoch 2750: 2.044172820425942\n",
            "Cost after epoch 2760: 2.0441775700417733\n",
            "Cost after epoch 2770: 2.0441815676006736\n",
            "Cost after epoch 2780: 2.044184767373686\n",
            "Cost after epoch 2790: 2.0441871180432503\n",
            "Cost after epoch 2800: 2.0441885688856605\n",
            "Cost after epoch 2810: 2.0441891032842268\n",
            "Cost after epoch 2820: 2.0441886810382375\n",
            "Cost after epoch 2830: 2.0441872520100164\n",
            "Cost after epoch 2840: 2.0441847691147697\n",
            "Cost after epoch 2850: 2.044181186289709\n",
            "Cost after epoch 2860: 2.0441765866198542\n",
            "Cost after epoch 2870: 2.0441708582217064\n",
            "Cost after epoch 2880: 2.0441639465032058\n",
            "Cost after epoch 2890: 2.0441558103637942\n",
            "Cost after epoch 2900: 2.0441464100387936\n",
            "Cost after epoch 2910: 2.0441359429892247\n",
            "Cost after epoch 2920: 2.0441242138814855\n",
            "Cost after epoch 2930: 2.0441111633409794\n",
            "Cost after epoch 2940: 2.044096757353518\n",
            "Cost after epoch 2950: 2.044080963446281\n",
            "Cost after epoch 2960: 2.0440641036457388\n",
            "Cost after epoch 2970: 2.044045890079185\n",
            "Cost after epoch 2980: 2.044026258099405\n",
            "Cost after epoch 2990: 2.0440051813090734\n",
            "Cost after epoch 3000: 2.043982634961016\n",
            "Cost after epoch 3010: 2.043959071350101\n",
            "Cost after epoch 3020: 2.043934103935291\n",
            "Cost after epoch 3030: 2.043907662252933\n",
            "Cost after epoch 3040: 2.0438797275772065\n",
            "Cost after epoch 3050: 2.0438502828593017\n",
            "Cost after epoch 3060: 2.043819912122296\n",
            "Cost after epoch 3070: 2.043788127050207\n",
            "Cost after epoch 3080: 2.043754850854455\n",
            "Cost after epoch 3090: 2.0437200721611632\n",
            "Cost after epoch 3100: 2.043683781234461\n",
            "Cost after epoch 3110: 2.0436466914616798\n",
            "Cost after epoch 3120: 2.0436082125541124\n",
            "Cost after epoch 3130: 2.043568260992576\n",
            "Cost after epoch 3140: 2.043526832160535\n",
            "Cost after epoch 3150: 2.0434839229905437\n",
            "Cost after epoch 3160: 2.0434403705796775\n",
            "Cost after epoch 3170: 2.043395485130744\n",
            "Cost after epoch 3180: 2.043349176137113\n",
            "Cost after epoch 3190: 2.043301444960971\n",
            "Cost after epoch 3200: 2.0432522943896276\n",
            "Cost after epoch 3210: 2.043202676878212\n",
            "Cost after epoch 3220: 2.0431518079089703\n",
            "Cost after epoch 3230: 2.04309958990604\n",
            "Cost after epoch 3240: 2.0430460293270176\n",
            "Cost after epoch 3250: 2.0429911339087683\n",
            "Cost after epoch 3260: 2.0429359610249667\n",
            "Cost after epoch 3270: 2.0428796381562195\n",
            "Cost after epoch 3280: 2.04282206075975\n",
            "Cost after epoch 3290: 2.0427632394663293\n",
            "Cost after epoch 3300: 2.042703186029367\n",
            "Cost after epoch 3310: 2.0426430508048\n",
            "Cost after epoch 3320: 2.042581881506039\n",
            "Cost after epoch 3330: 2.042519566904767\n",
            "Cost after epoch 3340: 2.0424561208955514\n",
            "Cost after epoch 3350: 2.0423915583370875\n",
            "Cost after epoch 3360: 2.0423271096822186\n",
            "Cost after epoch 3370: 2.042261752214365\n",
            "Cost after epoch 3380: 2.04219536846434\n",
            "Cost after epoch 3390: 2.04212797473227\n",
            "Cost after epoch 3400: 2.042059588128805\n",
            "Cost after epoch 3410: 2.0419915058572102\n",
            "Cost after epoch 3420: 2.041922644820212\n",
            "Cost after epoch 3430: 2.0418528818924475\n",
            "Cost after epoch 3440: 2.04178223499867\n",
            "Cost after epoch 3450: 2.041710722730305\n",
            "Cost after epoch 3460: 2.0416396956523184\n",
            "Cost after epoch 3470: 2.041568020693266\n",
            "Cost after epoch 3480: 2.0414955697685673\n",
            "Cost after epoch 3490: 2.0414223617397447\n",
            "Cost after epoch 3500: 2.0413484160040207\n",
            "Cost after epoch 3510: 2.041275123403645\n",
            "Cost after epoch 3520: 2.0412013113463034\n",
            "Cost after epoch 3530: 2.0411268475677504\n",
            "Cost after epoch 3540: 2.041051751278766\n",
            "Cost after epoch 3550: 2.0409760421094996\n",
            "Cost after epoch 3560: 2.0409011386745837\n",
            "Cost after epoch 3570: 2.040825839099265\n",
            "Cost after epoch 3580: 2.0407500077722798\n",
            "Cost after epoch 3590: 2.0406736637672314\n",
            "Cost after epoch 3600: 2.0405968264760586\n",
            "Cost after epoch 3610: 2.0405209305685976\n",
            "Cost after epoch 3620: 2.040444754699285\n",
            "Cost after epoch 3630: 2.0403681607678497\n",
            "Cost after epoch 3640: 2.0402911673212856\n",
            "Cost after epoch 3650: 2.040213793139014\n",
            "Cost after epoch 3660: 2.0401374781641515\n",
            "Cost after epoch 3670: 2.0400609908013085\n",
            "Cost after epoch 3680: 2.0399841913196894\n",
            "Cost after epoch 3690: 2.0399070974399143\n",
            "Cost after epoch 3700: 2.039829727043488\n",
            "Cost after epoch 3710: 2.039753515594499\n",
            "Cost after epoch 3720: 2.0396772297588184\n",
            "Cost after epoch 3730: 2.0396007290127223\n",
            "Cost after epoch 3740: 2.0395240300310187\n",
            "Cost after epoch 3750: 2.039447149591088\n",
            "Cost after epoch 3760: 2.0393715100044596\n",
            "Cost after epoch 3770: 2.0392958839201487\n",
            "Cost after epoch 3780: 2.0392201308206643\n",
            "Cost after epoch 3790: 2.0391442661855637\n",
            "Cost after epoch 3800: 2.039068305550533\n",
            "Cost after epoch 3810: 2.0389936504916473\n",
            "Cost after epoch 3820: 2.0389190865291034\n",
            "Cost after epoch 3830: 2.0388444738986675\n",
            "Cost after epoch 3840: 2.0387698267946477\n",
            "Cost after epoch 3850: 2.038695159431445\n",
            "Cost after epoch 3860: 2.038621846139452\n",
            "Cost after epoch 3870: 2.0385486913487774\n",
            "Cost after epoch 3880: 2.038475556733292\n",
            "Cost after epoch 3890: 2.0384024551612394\n",
            "Cost after epoch 3900: 2.0383293994938647\n",
            "Cost after epoch 3910: 2.0382577313373655\n",
            "Cost after epoch 3920: 2.0381862792295142\n",
            "Cost after epoch 3930: 2.0381149068982594\n",
            "Cost after epoch 3940: 2.038043625884577\n",
            "Cost after epoch 3950: 2.0379724477028804\n",
            "Cost after epoch 3960: 2.037902676730493\n",
            "Cost after epoch 3970: 2.037833169996687\n",
            "Cost after epoch 3980: 2.0377637938268403\n",
            "Cost after epoch 3990: 2.037694558464391\n",
            "Cost after epoch 4000: 2.0376254741128697\n",
            "Cost after epoch 4010: 2.0375578043202793\n",
            "Cost after epoch 4020: 2.037490438218539\n",
            "Cost after epoch 4030: 2.0374232451999283\n",
            "Cost after epoch 4040: 2.0373562342633567\n",
            "Cost after epoch 4050: 2.037289414359512\n",
            "Cost after epoch 4060: 2.03722400543396\n",
            "Cost after epoch 4070: 2.037158931611064\n",
            "Cost after epoch 4080: 2.0370940657463987\n",
            "Cost after epoch 4090: 2.037029415664027\n",
            "Cost after epoch 4100: 2.0369649891354578\n",
            "Cost after epoch 4110: 2.036901960477458\n",
            "Cost after epoch 4120: 2.036839291033236\n",
            "Cost after epoch 4130: 2.0367768574474105\n",
            "Cost after epoch 4140: 2.0367146664499836\n",
            "Cost after epoch 4150: 2.036652724717142\n",
            "Cost after epoch 4160: 2.0365921595758723\n",
            "Cost after epoch 4170: 2.0365319712129395\n",
            "Cost after epoch 4180: 2.036472040320769\n",
            "Cost after epoch 4190: 2.036412372622597\n",
            "Cost after epoch 4200: 2.036352973788866\n",
            "Cost after epoch 4210: 2.036294923381293\n",
            "Cost after epoch 4220: 2.036237261516266\n",
            "Cost after epoch 4230: 2.0361798731303526\n",
            "Cost after epoch 4240: 2.03612276303011\n",
            "Cost after epoch 4250: 2.0360659359719566\n",
            "Cost after epoch 4260: 2.036010423485624\n",
            "Cost after epoch 4270: 2.0359553062270668\n",
            "Cost after epoch 4280: 2.0359004735160346\n",
            "Cost after epoch 4290: 2.0358459293323676\n",
            "Cost after epoch 4300: 2.0357916776095273\n",
            "Cost after epoch 4310: 2.035738702014975\n",
            "Cost after epoch 4320: 2.0356861239386057\n",
            "Cost after epoch 4330: 2.0356338371705656\n",
            "Cost after epoch 4340: 2.0355818449515937\n",
            "Cost after epoch 4350: 2.035530150480482\n",
            "Cost after epoch 4360: 2.0354796901014347\n",
            "Cost after epoch 4370: 2.035429625774612\n",
            "Cost after epoch 4380: 2.0353798558015366\n",
            "Cost after epoch 4390: 2.035330382767483\n",
            "Cost after epoch 4400: 2.0352812092205443\n",
            "Cost after epoch 4410: 2.035233225028528\n",
            "Cost after epoch 4420: 2.0351856322538806\n",
            "Cost after epoch 4430: 2.0351383337097904\n",
            "Cost after epoch 4440: 2.035091331404603\n",
            "Cost after epoch 4450: 2.0350446273143117\n",
            "Cost after epoch 4460: 2.034999065929478\n",
            "Cost after epoch 4470: 2.0349538886920686\n",
            "Cost after epoch 4480: 2.0349090028919115\n",
            "Cost after epoch 4490: 2.0348644100331357\n",
            "Cost after epoch 4500: 2.0348201115922238\n",
            "Cost after epoch 4510: 2.034776907984278\n",
            "Cost after epoch 4520: 2.0347340790982495\n",
            "Cost after epoch 4530: 2.034691536635393\n",
            "Cost after epoch 4540: 2.0346492816621238\n",
            "Cost after epoch 4550: 2.0346073152216704\n",
            "Cost after epoch 4560: 2.0345663951141963\n",
            "Cost after epoch 4570: 2.0345258385738165\n",
            "Cost after epoch 4580: 2.0344855616225783\n",
            "Cost after epoch 4590: 2.0344455649493813\n",
            "Cost after epoch 4600: 2.034405849224044\n",
            "Cost after epoch 4610: 2.0343671312126355\n",
            "Cost after epoch 4620: 2.0343287642593118\n",
            "Cost after epoch 4630: 2.034290668595282\n",
            "Cost after epoch 4640: 2.0342528445859047\n",
            "Cost after epoch 4650: 2.034215292581156\n",
            "Cost after epoch 4660: 2.0341786899808825\n",
            "Cost after epoch 4670: 2.0341424249025333\n",
            "Cost after epoch 4680: 2.0341064216578766\n",
            "Cost after epoch 4690: 2.034070680336717\n",
            "Cost after epoch 4700: 2.034035201016758\n",
            "Cost after epoch 4710: 2.0340006234579637\n",
            "Cost after epoch 4720: 2.0339663691403027\n",
            "Cost after epoch 4730: 2.0339323663141435\n",
            "Cost after epoch 4740: 2.0338986148360987\n",
            "Cost after epoch 4750: 2.0338651145535396\n",
            "Cost after epoch 4760: 2.03383246934708\n",
            "Cost after epoch 4770: 2.033800132598239\n",
            "Cost after epoch 4780: 2.0337680363438886\n",
            "Cost after epoch 4790: 2.033736180244614\n",
            "Cost after epoch 4800: 2.033704563954204\n",
            "Cost after epoch 4810: 2.033673757248232\n",
            "Cost after epoch 4820: 2.033643243918954\n",
            "Cost after epoch 4830: 2.0336129596303736\n",
            "Cost after epoch 4840: 2.0335829038794615\n",
            "Cost after epoch 4850: 2.033553076158465\n",
            "Cost after epoch 4860: 2.0335240139089934\n",
            "Cost after epoch 4870: 2.033495229830632\n",
            "Cost after epoch 4880: 2.033466663050328\n",
            "Cost after epoch 4890: 2.0334383129296003\n",
            "Cost after epoch 4900: 2.0334101788269723\n",
            "Cost after epoch 4910: 2.033382767603904\n",
            "Cost after epoch 4920: 2.033355619365767\n",
            "Cost after epoch 4930: 2.0333286765355973\n",
            "Cost after epoch 4940: 2.0333019383638375\n",
            "Cost after epoch 4950: 2.0332754040993657\n",
            "Cost after epoch 4960: 2.0332495517486384\n",
            "Cost after epoch 4970: 2.0332239473351112\n",
            "Cost after epoch 4980: 2.0331985364102216\n",
            "Cost after epoch 4990: 2.0331733181343896\n",
            "training time: 0:00:01.168796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCgVrwWd8l95"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiqeRToV8l96"
      },
      "source": [
        "X_test = np.arange(vocab_size)\n",
        "X_test = np.expand_dims(X_test, axis=0)\n",
        "softmax_test, _ = forward_propagation(X_test, paras)\n",
        "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiRKqe6w8l96",
        "outputId": "d46e96e8-e198-447f-e94e-976bccd2def0"
      },
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    input_word = id_to_word[input_ind]\n",
        "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
        "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "investing's neighbor words: ['the', 'stock', 'beating', \"loser's\"]\n",
            "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
            "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
            "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
            "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
            "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
            "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
            "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
            "beating's neighbor words: ['market', 'stock', 'investing', 'costs']\n",
            "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
            "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
            "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
            "stock's neighbor words: ['investing', 'a', 'is', 'beating']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}